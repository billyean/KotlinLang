# Create a file

## Process Explain

### Client create a new file.

* [DFSClient](https://github.com/apache/hadoop/blob/trunk/hadoop-hdfs-project/hadoop-hdfs-client/src/main/java/org/apache/hadoop/hdfs/DFSClient.java) creates a client instance, then make a [create](https://github.com/apache/hadoop/blob/trunk/hadoop-hdfs-project/hadoop-hdfs-client/src/main/java/org/apache/hadoop/hdfs/DFSClient.java#create) call to create a file in HDFS. 
* create method get mode by given permission or default permission if not given.
* create method call [DFSOutputStream.newStreamForCreate](https://github.com/apache/hadoop/blob/trunk/hadoop-hdfs-project/hadoop-hdfs-client/src/main/java/org/apache/hadoop/hdfs/DFSOutputStream.java) method to acquire a [DFSOutputStream](https://github.com/apache/hadoop/blob/trunk/hadoop-hdfs-project/hadoop-hdfs-client/src/main/java/org/apache/hadoop/hdfs/DFSOutputStream.java) instance.
* after client acquired the [DFSOutputStream](https://github.com/apache/hadoop/blob/trunk/hadoop-hdfs-project/hadoop-hdfs-client/src/main/java/org/apache/hadoop/hdfs/DFSOutputStream.java) instance, it calls [beginFileLease](https://github.com/apache/hadoop/blob/trunk/hadoop-hdfs-project/hadoop-hdfs-client/src/main/java/org/apache/hadoop/hdfs/DFSClient.java#beginFileLease) to start automatic renewal process. [LeaseRenewer](https://github.com/apache/hadoop/blob/trunk/hadoop-hdfs-project/hadoop-hdfs-client/src/main/java/org/apache/hadoop/hdfs/client/impl/LeaseRenewer.java) is a daemon thread to check if the renewal is expired. 
* if Erasure Coding is enabled, a DFSStripedOutputStream instance will be created, otherwise a DFSOutputStream instance will be created(Data Streamer instance will be created when EC is not enabled).
* Once DFSOutputStream instance created, DFSClient computes how many chunks and packets will be sent.
* When EC is not enabled, [DataStreamer](https://github.com/apache/hadoop/blob/trunk/hadoop-hdfs-project/hadoop-hdfs-client/src/main/java/org/apache/hadoop/hdfs/DataStreamer.java) will take the responsibilities for sending data packets to the datanodes in the pipeline.

### Communication protocol
* [DFSClient](https://github.com/apache/hadoop/blob/trunk/hadoop-hdfs-project/hadoop-hdfs-client/src/main/java/org/apache/hadoop/hdfs/DFSClient.java) holds a retry policy if 10 times retries are not reached.
* DFSClient uses [ClientProtocol](https://github.com/apache/hadoop/blob/trunk/hadoop-hdfs-project/hadoop-hdfs-client/src/main/java/org/apache/hadoop/hdfs/protocol/ClientProtocol.java) to make a RPC call.
* [NameNodeRpcServer](https://github.com/apache/hadoop/blob/trunk/hadoop-hdfs-project/hadoop-hdfs/src/main/java/org/apache/hadoop/hdfs/server/namenode/NameNodeRpcServer.java) takes the RPC call from DFSClient. 

### Prepare for new file.

* Once [NameNodeRpcServer](https://github.com/apache/hadoop/blob/trunk/hadoop-hdfs-project/hadoop-hdfs/src/main/java/org/apache/hadoop/hdfs/server/namenode/NameNodeRpcServer.java) get create RPC call from DFSClient, first it will check if name node is up and do sanity check for create file path length. It calls [FSNamesystem.startFile](https://github.com/apache/hadoop/blob/trunk/hadoop-hdfs-project/hadoop-hdfs/src/main/java/org/apache/hadoop/hdfs/server/namenode/FSNamesystem.java#startFile) to create a new file.
* [FSNamesystem.startFile](https://github.com/apache/hadoop/blob/trunk/hadoop-hdfs-project/hadoop-hdfs/src/main/java/org/apache/hadoop/hdfs/server/namenode/FSNamesystem.java#startFile) will call [FSNamesystem.startFileInt](https://github.com/apache/hadoop/blob/trunk/hadoop-hdfs-project/hadoop-hdfs/src/main/java/org/apache/hadoop/hdfs/server/namenode/FSNamesystem.java#startFileInt) to create file.
* [FSNamesystem.startFileInt](https://github.com/apache/hadoop/blob/trunk/hadoop-hdfs-project/hadoop-hdfs/src/main/java/org/apache/hadoop/hdfs/server/namenode/FSNamesystem.java#startFileInt) starts with checking validity of the given file path and permission, then it acquires a R/W lock(ReentrantReadWriteLock) for writing the data.
* [FSNamesystem.startFileInt](https://github.com/apache/hadoop/blob/trunk/hadoop-hdfs-project/hadoop-hdfs/src/main/java/org/apache/hadoop/hdfs/server/namenode/FSNamesystem.java#startFileInt) also checks name node is not in safe mode, create inode for the file, check if replication factor when EC is not enabled, check block size is valid, acquires FileEncryptionInfo if EZ policy is given. 
* At the end [FSNamesystem.startFileInt](https://github.com/apache/hadoop/blob/trunk/hadoop-hdfs-project/hadoop-hdfs/src/main/java/org/apache/hadoop/hdfs/server/namenode/FSNamesystem.java#startFileInt) checks if the associated path existed, create path if the path doesn't exist by calling [FSDirWriteFileOp.startFile](https://github.com/apache/hadoop/blob/trunk/hadoop-hdfs-project/hadoop-hdfs/src/main/java/org/apache/hadoop/hdfs/server/namenode/FSDirWriteFileOp.java#startFile).
* [FSNamesystem.startFileInt](https://github.com/apache/hadoop/blob/trunk/hadoop-hdfs-project/hadoop-hdfs/src/main/java/org/apache/hadoop/hdfs/server/namenode/FSNamesystem.java#startFileInt) write edit log for file creation event when skipSync flag is false. 
* After calling [FSNamesystem.startFileInt](https://github.com/apache/hadoop/blob/trunk/hadoop-hdfs-project/hadoop-hdfs/src/main/java/org/apache/hadoop/hdfs/server/namenode/FSNamesystem.java#startFileInt), [NameNodeRpcServer](https://github.com/apache/hadoop/blob/trunk/hadoop-hdfs-project/hadoop-hdfs/src/main/java/org/apache/hadoop/hdfs/server/namenode/NameNodeRpcServer.java) will add audit log for file creation event.(Audit log is human readable log)
* [NameNodeRpcServer](https://github.com/apache/hadoop/blob/trunk/hadoop-hdfs-project/hadoop-hdfs/src/main/java/org/apache/hadoop/hdfs/server/namenode/NameNodeRpcServer.java) returns a [HdfsFileStatus](https://github.com/apache/hadoop/blob/trunk/hadoop-hdfs-project/hadoop-hdfs-client/src/main/java/org/apache/hadoop/hdfs/protocol/HdfsFileStatus.java) instance once the file created successfully.

### Writing

* When EC is not enabled, [DataStreamer](https://github.com/apache/hadoop/blob/trunk/hadoop-hdfs-project/hadoop-hdfs-client/src/main/java/org/apache/hadoop/hdfs/DataStreamer.java) will take the responsibilities for sending data packets to the datanodes in the pipeline
* [DataStreamer](https://github.com/apache/hadoop/blob/trunk/hadoop-hdfs-project/hadoop-hdfs-client/src/main/java/org/apache/hadoop/hdfs/DataStreamer.java) is a daemon thread that opens streams to data node, and closes them.
* Every DataStreamer hold one dataQueue.
* DataStreamer periodically checks dataQueue until steamer closed or dfsClient.clientRunning is false(calling DataStreamer.close will set false).
* DataStreamer will send heart beat packet if no data is available in dataQueue. 
* DataStreamer thread also will force itself to sleep if writing pipeline is congested.
* when [DFSOutputStream.writeChunk](hadoop-hdfs-project/hadoop-hdfs-client/src/main/java/org/apache/hadoop/hdfs/DFSOutputStream.java#writeChunk) is called for writing the data, it calls enqueueCurrentPacketFull method thats calls [DataStreamer.waitAndQueuePacket](https://github.com/apache/hadoop/blob/trunk/hadoop-hdfs-project/hadoop-hdfs-client/src/main/java/org/apache/hadoop/hdfs/DataStreamer.java#waitAndQueuePacket) to add packet to the queue, which means the packets are not sent to data node right away. Instead DataStreamer will read packets from the queue and send to data node.

#### add block.
* Once DataStreamer read data from dataQueue, it first checks current stage of writing. If current stage is PIPELINE_SETUP_CREATE, DataStreamer calls [nextBlockOutputStream](https://github.com/apache/hadoop/blob/trunk/hadoop-hdfs-project/hadoop-hdfs-client/src/main/java/org/apache/hadoop/hdfs/DataStreamer.java#nextBlockOutputStream) to create a new block
* [nextBlockOutputStream](https://github.com/apache/hadoop/blob/trunk/hadoop-hdfs-project/hadoop-hdfs-client/src/main/java/org/apache/hadoop/hdfs/DataStreamer.java#nextBlockOutputStream) calls [locateFollowingBlock](https://github.com/apache/hadoop/blob/trunk/hadoop-hdfs-project/hadoop-hdfs-client/src/main/java/org/apache/hadoop/hdfs/DataStreamer.java#locateFollowingBlock) then [DFSOutputStream.addBlock](hadoop-hdfs-project/hadoop-hdfs-client/src/main/java/org/apache/hadoop/hdfs/DFSOutputStream.java#addBlock) that create a new block.
* [DFSOutputStream.addBlock](hadoop-hdfs-project/hadoop-hdfs-client/src/main/java/org/apache/hadoop/hdfs/DFSOutputStream.java#addBlock) makes a RPC call to [NameNodeRpcServer.addBlock](https://github.com/apache/hadoop/blob/trunk/hadoop-hdfs-project/hadoop-hdfs/src/main/java/org/apache/hadoop/hdfs/server/namenode/NameNodeRpcServer.java#addBlock), [NameNodeRpcServer.addBlock](https://github.com/apache/hadoop/blob/trunk/hadoop-hdfs-project/hadoop-hdfs/src/main/java/org/apache/hadoop/hdfs/server/namenode/NameNodeRpcServer.java#addBlock) calls [FSNamesystem.getAdditionalBlock](https://github.com/apache/hadoop/blob/trunk/hadoop-hdfs-project/hadoop-hdfs/src/main/java/org/apache/hadoop/hdfs/server/namenode/FSNamesystem.java#getAdditionalBlock) to add a new block.  
* [FSNamesystem.getAdditionalBlock](https://github.com/apache/hadoop/blob/trunk/hadoop-hdfs-project/hadoop-hdfs/src/main/java/org/apache/hadoop/hdfs/server/namenode/FSNamesystem.java#getAdditionalBlock) acquires a read lock first, then calls [FSDirWriteFileOp.validateAddBlock](https://github.com/apache/hadoop/blob/trunk/hadoop-hdfs-project/hadoop-hdfs/src/main/java/org/apache/hadoop/hdfs/server/namenode/FSDirWriteFileOp.java#validateAddBlock) to create a new block.
* [FSDirWriteFileOp.validateAddBlock](https://github.com/apache/hadoop/blob/trunk/hadoop-hdfs-project/hadoop-hdfs/src/main/java/org/apache/hadoop/hdfs/server/namenode/FSDirWriteFileOp.java#validateAddBlock) acquires inode first, then checks if previous block has been replicated, it also check if the current file has reached the up limit of file size.
* [FSNamesystem.getAdditionalBlock](https://github.com/apache/hadoop/blob/trunk/hadoop-hdfs-project/hadoop-hdfs/src/main/java/org/apache/hadoop/hdfs/server/namenode/FSNamesystem.java#getAdditionalBlock) calls [FSDirWriteFileOp.chooseTargetForNewBlock](https://github.com/apache/hadoop/blob/trunk/hadoop-hdfs-project/hadoop-hdfs/src/main/java/org/apache/hadoop/hdfs/server/namenode/FSDirWriteFileOp.java#chooseTargetForNewBlock) to select target data nodes that will be written.
* [BlockManager.chooseTarget4NewBlock](https://github.com/apache/hadoop/blob/trunk/hadoop-hdfs-project/hadoop-hdfs/src/main/java/org/apache/hadoop/hdfs/server/blockmanagement/BlockManager.java#chooseTarget4NewBlock) will be invoked to identify the data nodes that will be written. It uses BlockPlacementPolicy decide the target data nodes, it considers factor of replicator numbers and excluded nodes.
* There are four provided BlockPlacementPolicy - [BlockPlacementPolicyDefault](https://github.com/apache/hadoop/blob/trunk/hadoop-hdfs-project/hadoop-hdfs/src/main/java/org/apache/hadoop/hdfs/server/blockmanagement/BlockPlacementPolicyDefault.java), [BlockPlacementPolicyRackFaultTolerant](https://github.com/apache/hadoop/blob/trunk/hadoop-hdfs-project/hadoop-hdfs/src/main/java/org/apache/hadoop/hdfs/server/blockmanagement/BlockPlacementPolicyRackFaultTolerant.java), [BlockPlacementPolicyWithNodeGroup](https://github.com/apache/hadoop/blob/trunk/hadoop-hdfs-project/hadoop-hdfs/src/main/java/org/apache/hadoop/hdfs/server/blockmanagement/BlockPlacementPolicyWithNodeGroup.java) and [BlockPlacementPolicyWithUpgradeDomain](https://github.com/apache/hadoop/blob/trunk/hadoop-hdfs-project/hadoop-hdfs/src/main/java/org/apache/hadoop/hdfs/server/blockmanagement/BlockPlacementPolicyWithUpgradeDomain.java). Please refer to [Different BlockPlacementPolicy](./BlockPlacementPolicy.md) to see the discussion of BlockPlacementPolicy.
* Once the target data nodes have been decided, [FSNamesystem.getAdditionalBlock](https://github.com/apache/hadoop/blob/trunk/hadoop-hdfs-project/hadoop-hdfs/src/main/java/org/apache/hadoop/hdfs/server/namenode/FSNamesystem.java#getAdditionalBlock) acquire a R/W lock then choose final target locations. If everything is OK, [FSDirWriteFileOp.addBlock](https://github.com/apache/hadoop/blob/trunk/hadoop-hdfs-project/hadoop-hdfs/src/main/java/org/apache/hadoop/hdfs/server/namenode/FSDirWriteFileOp.java#addBlock) will be called.
* [FSDirWriteFileOp.storeAllocatedBlock](https://github.com/apache/hadoop/blob/trunk/hadoop-hdfs-project/hadoop-hdfs/src/main/java/org/apache/hadoop/hdfs/server/namenode/FSDirWriteFileOp.java#storeAllocatedBlock) acquires a R/W lock on FSDirectory first, then it associates the new last block for the file(By using inode).
* If erasure coding is enabled, FSDirectory will be updated by number of locations(number of data units + number of parity units), otherwise FSDirectory will be updated by replicator number.
* [BlockManager.addBlockCollection](https://github.com/apache/hadoop/blob/trunk/hadoop-hdfs-project/hadoop-hdfs/src/main/java/org/apache/hadoop/hdfs/server/blockmanagement/BlockManager.java#addBlockCollection) will be invoked to add block to [BlocksMap](https://github.com/apache/hadoop/blob/trunk/hadoop-hdfs-project/hadoop-hdfs/src/main/java/org/apache/hadoop/hdfs/server/blockmanagement/BlocksMap.java), [BlocksMap](https://github.com/apache/hadoop/blob/trunk/hadoop-hdfs-project/hadoop-hdfs/src/main/java/org/apache/hadoop/hdfs/server/blockmanagement/BlocksMap.java) maintains the map from a block to its metadata that includes the local file system path. Also [FSDirWriteFileOp.addBlock](https://github.com/apache/hadoop/blob/trunk/hadoop-hdfs-project/hadoop-hdfs/src/main/java/org/apache/hadoop/hdfs/server/namenode/FSDirWriteFileOp.java#addBlock) calls [INodeFile.addBlock](https://github.com/apache/hadoop/blob/trunk/hadoop-hdfs-project/hadoop-hdfs/src/main/java/org/apache/hadoop/hdfs/server/namenode/INodeFile.java#addBlock) adds block info to inode file.
* [FSDirWriteFileOp.persistNewBlock](https://github.com/apache/hadoop/blob/trunk/hadoop-hdfs-project/hadoop-hdfs/src/main/java/org/apache/hadoop/hdfs/server/namenode/FSDirWriteFileOp.java#persistNewBlock) will be called to write the edit log.
* Once all above operations succeed, a LocatedBlock(block with the Datanodes that contain its replicas  and other block metadata ) instance will be returned to client.

* after [DataStreamer](https://github.com/apache/hadoop/blob/trunk/hadoop-hdfs-project/hadoop-hdfs-client/src/main/java/org/apache/hadoop/hdfs/DataStreamer.java) gets new block info, it calls initDataStreaming to start to write data. It first initiate a [ResponseProcessor](https://github.com/apache/hadoop/blob/trunk/hadoop-hdfs-project/hadoop-hdfs-client/src/main/java/org/apache/hadoop/hdfs/DataStreamer.java#ResponseProcessor) instance to process responses from the data node. then set stage to DATA_STREAMING.
* [DataStreamer](https://github.com/apache/hadoop/blob/trunk/hadoop-hdfs-project/hadoop-hdfs-client/src/main/java/org/apache/hadoop/hdfs/DataStreamer.java) keeps reading dataQueue and add to ackQueue.
* [DFSPacket.writeTo](https://github.com/apache/hadoop/blob/trunk/hadoop-hdfs-project/hadoop-hdfs-client/src/main/java/org/apache/hadoop/hdfs/DFSPacket.java) is responsible for writing the data to [DFSOutputStream](hadoop-hdfs-project/hadoop-hdfs-client/src/main/java/org/apache/hadoop/hdfs/DFSOutputStream.java)

* [DataXceiver](./dn/DataXceiver.md) is the thread in data node processing the data.

### close
* Once DFSClient calls close on DFSOutputStream. It first make sure current into DataStreamer's dataQueue, then flushes data to name node by calling [waitForAckedSeqno](https://github.com/apache/hadoop/blob/trunk/hadoop-hdfs-project/hadoop-hdfs-client/src/main/java/org/apache/hadoop/hdfs/DFSClient.java#waitForAckedSeqno)(Wait until all data has been acknowledged by data node). 
* DFSClient will stop its associated [DataStreamer](https://github.com/apache/hadoop/blob/trunk/hadoop-hdfs-project/hadoop-hdfs-client/src/main/java/org/apache/hadoop/hdfs/DataStreamer.java) thread and close its socket, eventually set close flag as true.
* After above, DFSClient calls [closeConnectionToNamenode](https://github.com/apache/hadoop/blob/trunk/hadoop-hdfs-project/hadoop-hdfs-client/src/main/java/org/apache/hadoop/hdfs/DFSClient.java#closeConnectionToNamenode) to close connections to the namenode. Which will close rpc proxy to the name node.